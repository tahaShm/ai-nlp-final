{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showProgressPercentage(current, total) : \n",
    "    prevPercentage = int ((current-1) / total * 100)\n",
    "    currPrecentage = int (current / total * 100)\n",
    "    progress = 0\n",
    "    if (currPrecentage >= 90 and prevPercentage < 90) : \n",
    "        progress = 90\n",
    "    elif (currPrecentage >= 80 and prevPercentage < 80) : \n",
    "        progress = 80 \n",
    "    elif (currPrecentage >= 70 and prevPercentage < 70) : \n",
    "        progress = 70 \n",
    "    elif (currPrecentage >= 60 and prevPercentage < 60) : \n",
    "        progress = 60 \n",
    "    elif (currPrecentage >= 50 and prevPercentage < 50) : \n",
    "        progress = 50 \n",
    "    elif (currPrecentage >= 40 and prevPercentage < 40) : \n",
    "        progress = 40 \n",
    "    elif (currPrecentage >= 30 and prevPercentage < 30) : \n",
    "        progress = 30 \n",
    "    elif (currPrecentage >= 20 and prevPercentage < 20) : \n",
    "        progress = 20 \n",
    "    elif (currPrecentage >= 10 and prevPercentage < 10) : \n",
    "        progress = 10 \n",
    "    if (progress > 0) :\n",
    "        print(progress, \"%\")\n",
    "        \n",
    "def mapDayToNumber(day) :\n",
    "    dayOfWeeks = [\"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]\n",
    "    for i in range(len(dayOfWeeks)) : \n",
    "        if (dayOfWeeks[i] == day) : \n",
    "            return i+1\n",
    "    return 0\n",
    "def mapTimeToNumber(time) :\n",
    "    times = [\"12AM\", \"01AM\", \"02AM\", \"03AM\", \"04AM\", \"05AM\", \"06AM\", \"07AM\", \"08AM\", \"09AM\", \"10AM\", \"11AM\", \"12PM\", \"01PM\", \"02PM\", \"03PM\", \"04PM\", \"05PM\", \"06PM\", \"07PM\", \"08PM\", \"09PM\", \"10PM\", \"11PM\"]\n",
    "    for i in range(len(times)) : \n",
    "        if (times[i] == time) : \n",
    "            return i\n",
    "    return 0\n",
    "    \n",
    "def fetchDay(createTime)  :\n",
    "    [dayOfWeek, time] = createTime.split(' ')\n",
    "    return mapDayToNumber(dayOfWeek)\n",
    "\n",
    "def fetchTime(createTime) : \n",
    "    [dayOfWeek, time] = createTime.split(' ')\n",
    "    return mapTimeToNumber(time)\n",
    "\n",
    "def correctThePrice(oldPrice) : \n",
    "    newPrice = oldPrice\n",
    "    if (4500 < oldPrice < 10000) : \n",
    "        newPrice = -11\n",
    "#         newPrice = oldPrice * 10\n",
    "    \n",
    "    elif (10 <= oldPrice <= 4500) : \n",
    "        newPrice = oldPrice * 1000\n",
    "        \n",
    "    elif (0 < oldPrice < 10) : \n",
    "        newPrice = -11\n",
    "#         newPrice = oldPrice * 1000000\n",
    "    \n",
    "    return newPrice\n",
    "\n",
    "def getAveragePricesPerBrands(df, brands) : \n",
    "    sums = { i : 0 for i in brands }\n",
    "    repeats = { i : 0 for i in brands }\n",
    "    avgs = { i : 0 for i in brands }\n",
    "    for index, dfRow in df.iterrows() :\n",
    "        price = dfRow['price']\n",
    "        if (price != -1) : \n",
    "            sums[dfRow['brand']] += price\n",
    "            repeats[dfRow['brand']] += 1\n",
    "    for brand in brands : \n",
    "        avgs[brand] = int(sums[brand] / repeats[brand])\n",
    "    return avgs\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProcessedNonTextData(dataFile, brandEncType = \"onehot\", cityEncType = \"onehot\") : \n",
    "    data = []\n",
    "    valData = []\n",
    "    targetData = []\n",
    "    col_list = [\"brand\", \"city\", \"title\", \"desc\", \"image_count\", \"created_at\", \"price\"]\n",
    "    df = pd.read_csv(dataFile, usecols=col_list)\n",
    "    \n",
    "    print(\"preprocessing started...\")\n",
    "    \n",
    "    if (brandEncType == \"onehot\") :\n",
    "        df = pd.concat([df,pd.get_dummies(df['brand'], prefix = \"brand\")],axis=1)\n",
    "    \n",
    "    print(\"brand onehot encoding done...\")\n",
    "        \n",
    "    if (cityEncType == \"onehot\") :\n",
    "        df = pd.concat([df,pd.get_dummies(df['city'], prefix = \"city\")],axis=1)\n",
    "        \n",
    "    print(\"city onehot encoding done...\")\n",
    "    \n",
    "    df['dayOfweek'] = df.apply(lambda row : fetchDay(row['created_at']), axis = 1) \n",
    "    df['time'] = df.apply(lambda row : fetchTime(row['created_at']), axis = 1) \n",
    "    del df['created_at']\n",
    "    \n",
    "    print(\"time preprocessing done...\")\n",
    "    \n",
    "    df['price'] = df.apply(lambda row : correctThePrice(row['price']), axis = 1)\n",
    "    df = df.loc[df['price'] > -10]\n",
    "    \n",
    "    brands = df.brand.unique()\n",
    "    avgPrices = getAveragePricesPerBrands(df, brands)\n",
    "    \n",
    "    print(\"price correction done, average price per every brand calculated...\")\n",
    "    \n",
    "    for index, dfRow in df.iterrows() :\n",
    "        price = dfRow['price']\n",
    "        if (price == -1) : \n",
    "            price = avgPrices[dfRow['brand']]\n",
    "        targetData.append(price)\n",
    "        del dfRow['price']\n",
    "        del dfRow['brand']\n",
    "        del dfRow['city']  \n",
    "        data.append(dfRow)\n",
    "    \n",
    "    for index, dfRow in df.iterrows() :\n",
    "        price = dfRow['price']\n",
    "        if (price == -1) : \n",
    "            price = avgPrices[dfRow['brand']]\n",
    "        targetData.append(price)\n",
    "        del dfRow['price']\n",
    "        del dfRow['brand']\n",
    "        del dfRow['city']  \n",
    "        del dfRow['title']\n",
    "        del dfRow['desc']\n",
    "        valData.append(dfRow)\n",
    "        \n",
    "    \n",
    "    labels = list(df)\n",
    "    labels.remove('price')\n",
    "    labels.remove('brand')\n",
    "    labels.remove('city')\n",
    "    valLabels = list(df)\n",
    "    valLabels.remove('price')\n",
    "    valLabels.remove('brand')\n",
    "    valLabels.remove('city')\n",
    "    valLabels.remove('title')\n",
    "    valLabels.remove('desc')\n",
    "    \n",
    "    print(\"part1 done.\")\n",
    "    return [df, data, valData, targetData, labels, valLabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing started...\n",
      "brand onehot encoding done...\n",
      "city onehot encoding done...\n",
      "time preprocessing done...\n",
      "price correction done, average price per every brand calculated...\n",
      "part1 done.\n"
     ]
    }
   ],
   "source": [
    "[df, data, valData, targetData, labels, valLabels] = getProcessedNonTextData(\"mobile_phone_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_count               2\n",
      "brand_Apple::اپل          0\n",
      "brand_HTC::اچ‌تی‌سی       0\n",
      "brand_Huawei::هوآوی       0\n",
      "brand_LG::ال‌جی           0\n",
      "brand_Lenovo::لنوو        0\n",
      "brand_Nokia::نوکیا        1\n",
      "brand_Samsung::سامسونگ    0\n",
      "brand_Sony::سونی          0\n",
      "brand_ZTE::زدتی‌ای        0\n",
      "city_Ahvaz                0\n",
      "city_Isfahan              0\n",
      "city_Karaj                0\n",
      "city_Kermanshah           0\n",
      "city_Mashhad              0\n",
      "city_Qom                  1\n",
      "city_Shiraz               0\n",
      "city_Tabriz               0\n",
      "city_Tehran               0\n",
      "dayOfweek                 5\n",
      "time                      7\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [59165, 118330]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-6ad902146277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minformationGains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutual_info_classif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshowInformationGains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minformationGains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_selection/_mutual_info.py\u001b[0m in \u001b[0;36mmutual_info_classif\u001b[0;34m(X, y, discrete_features, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     return _estimate_mi(X, y, discrete_features, True, n_neighbors,\n\u001b[0;32m--> 448\u001b[0;31m                         copy, random_state)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/feature_selection/_mutual_info.py\u001b[0m in \u001b[0;36m_estimate_mi\u001b[0;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state)\u001b[0m\n\u001b[1;32m    244\u001b[0m            \u001b[0mData\u001b[0m \u001b[0mSets\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mPLoS\u001b[0m \u001b[0mONE\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2014.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \"\"\"\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mdiscrete_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 257\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [59165, 118330]"
     ]
    }
   ],
   "source": [
    "informationGains = mutual_info_classif(valData, targetData)\n",
    "def showInformationGains(labels) :\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(informationGains) \n",
    "    plt.xticks(range(len(labels)), labels, rotation=90)\n",
    "    plt.xlabel('Features') \n",
    "    plt.ylabel('Gain') \n",
    "    plt.title('Information Gain') \n",
    "\n",
    "    plt.show()\n",
    "showInformationGains(valLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hazm\n",
    "from hazm.WordTokenizer import WordTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "from hazm.Lemmatizer import Lemmatizer\n",
    "from hazm.Normalizer import Normalizer\n",
    "import operator\n",
    "\n",
    "stop_words_fa = set(hazm.utils.stopwords_list()) \n",
    "stop_words_en = set(stopwords.words('english')) \n",
    "punctuations = {\",\", \".\", \"(\", \")\", \";\", \":\", \"؛\", \"،\"}\n",
    "stop_words = stop_words_fa.union(stop_words_en)\n",
    "stop_words = stop_words.union(punctuations)\n",
    "\n",
    "normalizer = Normalizer()\n",
    "tokenizer = WordTokenizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "# print(stop_words)\n",
    "\n",
    "def lowerAllCharacters(context) :\n",
    "    context = context.lower()\n",
    "    return context\n",
    "\n",
    "def normalizeContext(context) : \n",
    "    context = normalizer.affix_spacing(context)\n",
    "    context = normalizer.character_refinement(context)\n",
    "    context = normalizer.punctuation_spacing(context)\n",
    "    return context\n",
    "\n",
    "def removeStopWordsAndPunctuations(context) : \n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    context = tokenizer.tokenize(context)\n",
    "    \n",
    "    filtered_sentence = [] \n",
    "    \n",
    "    for w in context: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "def lemmatizeWords(wordList) : \n",
    "    for i in range(len(wordList)):\n",
    "        wordList[i] = lemmatizer.lemmatize(wordList[i])\n",
    "            \n",
    "    return wordList\n",
    "\n",
    "def getProcessedWords(context) : \n",
    "    context = lowerAllCharacters(context)\n",
    "    context = normalizeContext(context)\n",
    "#     wordList = []\n",
    "    wordList = removeStopWordsAndPunctuations(context)\n",
    "    wordList = lemmatizeWords(wordList)\n",
    "    return wordList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProcessedTextData(data, rowName) : \n",
    "    totalWords = {}\n",
    "    wordLists = []\n",
    "    for row in data :\n",
    "        wordList = getProcessedWords(row[rowName])\n",
    "        wordLists.append(wordList)\n",
    "        for word in wordList : \n",
    "            if word in totalWords : \n",
    "                totalWords[word] += 1\n",
    "            else :\n",
    "                totalWords[word] = 1\n",
    "#         print(i)\n",
    "#         print(row['title'])\n",
    "#         print(wordList)\n",
    "#         print(\"_____________________________________________________\")\n",
    "    return [wordLists, totalWords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[titleWordLists, totalTitleWords] = getProcessedTextData(data, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[descWordLists, totalDescWords] = getProcessedTextData(data, 'desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedTitleWords = sorted(totalTitleWords.items(),key=operator.itemgetter(1),reverse=True)\n",
    "sortedDescWords = sorted(totalDescWords.items(),key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "# idx = 0 #150\n",
    "# for i in sortedTitleWords :\n",
    "#     idx += 1\n",
    "#     print(idx, i)\n",
    "\n",
    "# idx = 0 #100\n",
    "# for i in sortedDescWords :\n",
    "#     idx += 1\n",
    "#     print(idx, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImpWords(wordList, amount) : \n",
    "    words = set()\n",
    "    i = 0\n",
    "    for word in wordList :\n",
    "        if (i >= amount) : \n",
    "            break\n",
    "        words.add(word[0])\n",
    "        i += 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleImpWords = getImpWords(sortedTitleWords, 45)\n",
    "descImpWords = getImpWords(sortedDescWords, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(titleImpWords)\n",
    "# print(descImpWords)\n",
    "def checkFeature(i, feature, wordList) : \n",
    "    if (feature in wordList[i]) : \n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowIndex(row):\n",
    "    return row.name\n",
    "def addFeatures(prefix, df, impWords, wordLists) :\n",
    "    i = 0\n",
    "    totalSize = len(impWords)\n",
    "    df[\"rowIndex\"] = list(range(len(df)))\n",
    "    for feature in impWords : \n",
    "        df[prefix + feature] = df.apply(lambda row : checkFeature(row['rowIndex'], feature, wordLists), axis = 1) \n",
    "        i += 1\n",
    "        showProgressPercentage(i, totalSize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = addFeatures(\"title_\", df, titleImpWords, titleWordLists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = addFeatures(\"desc_\", df, descImpWords, descWordLists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDatas(df) :\n",
    "    data = []\n",
    "    targetData = []\n",
    "    brands = df.brand.unique()\n",
    "    avgPrices = getAveragePricesPerBrands(df, brands)\n",
    "    \n",
    "    for index, dfRow in df.iterrows() :\n",
    "        price = dfRow['price']\n",
    "        if (price == -1) : \n",
    "            price = avgPrices[dfRow['brand']]\n",
    "        targetData.append(price)\n",
    "#         print(dfRow[\"desc_lla\"])\n",
    "        del dfRow['price']\n",
    "        del dfRow['brand']\n",
    "        del dfRow['city']\n",
    "        del dfRow['title']\n",
    "        del dfRow['desc']\n",
    "        data.append(dfRow)\n",
    "    \n",
    "    labels = list(df)\n",
    "    labels.remove('price')\n",
    "    labels.remove('brand')\n",
    "    labels.remove('city')\n",
    "    labels.remove('title')\n",
    "    labels.remove('desc')\n",
    "    print(\"preprocessing done.\")\n",
    "    return [df, data, targetData, labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing done.\n"
     ]
    }
   ],
   "source": [
    "[df2, data2, targetData2, labels2] = getDatas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "informationGains = mutual_info_classif(data, targetData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
